{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from functools import reduce\n",
    "import math\n",
    "import datetime as dt\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely.ops as so\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "CRS_NZGD49 = {'init': 'epsg:27200', 'no_defs': True}\n",
    "CRS_NZTM = {'init': 'epsg:2193', 'no_defs': True}\n",
    "CRS_WGS84 = {'init': 'epsg:4326'}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare area unit table\n",
    "\n",
    "path = DATA_DIR/'raw'/'Geographical Table.csv'\n",
    "f = pd.read_csv(path, dtype={'SAU': str})\n",
    "f = f.rename(columns={\n",
    "    'SAU': 'au2001', \n",
    "    'SAU.Desc': 'au_name', \n",
    "    'TA': 'territory',\n",
    "    'Region': 'region',\n",
    "})\n",
    "del f['Water']\n",
    "f.head()\n",
    "\n",
    "path = DATA_DIR/'raw'/'Market Rent Areas.csv'\n",
    "g = pd.read_csv(path, dtype={'SAU': str})\n",
    "g = g.rename(columns={\n",
    "    'SAU': 'au2001', \n",
    "    'MARKET RENT DESCRIPTION': 'rental_area',\n",
    "    'TA': 'territory',\n",
    "    'AU NAME': 'au_name',\n",
    "})\n",
    "\n",
    "# Clean rental areas\n",
    "def clean(x):\n",
    "    y = x.split(' - ')\n",
    "    y = y[1] if 'District' not in y[1] else y[0]\n",
    "    return y\n",
    "\n",
    "g['rental_area'] = g['rental_area'].map(clean)\n",
    "\n",
    "\n",
    "f = f.merge(g[['au2001', 'rental_area']])\n",
    "\n",
    "path = DATA_DIR/'au2001.csv'\n",
    "f.to_csv(str(path), index=False)\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare geodata as GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Shapefile\n",
    "\n",
    "path = DATA_DIR/'raw'/'NZ_AU01_region_simplified'/'NZ_AU01_region.shp'\n",
    "au = gpd.read_file(str(path))\n",
    "au.crs = CRS_NZGD49\n",
    "au = au.to_crs(CRS_WGS84)\n",
    "au = au.rename(columns={'AU01': 'au2001', 'AU_DESC': 'au_name'})\n",
    "print(au.shape)\n",
    "print(au.head())\n",
    "au.head().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove water area units\n",
    "\n",
    "pattern = r'ocean|strait|inlet|harbour'\n",
    "cond = au['au_name'].str.contains(pattern, case=False)\n",
    "au = au[~cond].copy()\n",
    "print(au.shape)\n",
    "au.head().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge geodata and metadata, drop null regions, and write to file\n",
    "\n",
    "path = DATA_DIR/'au2001.csv'\n",
    "f = pd.read_csv(path, dtype={'au2001': str})\n",
    "\n",
    "g = au.merge(f[['au2001', 'territory', 'region', 'rental_area']])\n",
    "g = g[g['region'].notnull()].copy()\n",
    "\n",
    "path = DATA_DIR/'au2001.geojson'\n",
    "with path.open('w') as tgt:\n",
    "    tgt.write(g.to_json())\n",
    "\n",
    "g.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create geodata for rental areas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve area units by area unit group\n",
    "\n",
    "path = DATA_DIR/'au2001.geojson'\n",
    "au = gpd.read_file(str(path))\n",
    "\n",
    "ra = au[['rental_area', 'region', 'territory', 'geometry']].dissolve(by='rental_area').reset_index()\n",
    "\n",
    "path = DATA_DIR/'rental_areas.geojson'\n",
    "with path.open('w') as tgt:\n",
    "    tgt.write(ra.to_json())\n",
    "\n",
    "ra.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare rent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and merge all rent data sets\n",
    "\n",
    "def clean(f, name):\n",
    "    f = f.copy()\n",
    "    f = f.rename(columns={\n",
    "        'SAU': 'au2001',\n",
    "        'Property_Type': 'property_type',\n",
    "        'Bedrooms': '#bedrooms'\n",
    "    })\n",
    "\n",
    "    # Drop subtotals\n",
    "    cond = False\n",
    "    for col in ['au2001', 'property_type', '#bedrooms']:\n",
    "        cond |= f[col].str.contains('total', case=False)\n",
    "\n",
    "    f = f[~cond].copy()\n",
    "    \n",
    "    # Reshape\n",
    "    id_vars = ['au2001', 'property_type', '#bedrooms']\n",
    "    value_vars = [c for c in f.columns if '-' in c]\n",
    "    f = pd.melt(f, id_vars=id_vars, value_vars=value_vars,\n",
    "      var_name='quarter', value_name=name)\n",
    "    \n",
    "    return f\n",
    "\n",
    "paths = [\n",
    "    DATA_DIR/'raw'/'Detailed Bonds Lodged.csv',\n",
    "    DATA_DIR/'raw'/'Detailed Mean Rents.csv',\n",
    "    DATA_DIR/'raw'/'Detailed Geomean Rents.csv',\n",
    "    DATA_DIR/'raw'/'Detailed Synthetic Lower Quartile Rents.csv',\n",
    "    DATA_DIR/'raw'/'Detailed Synthetic Upper Quartile Rents.csv',\n",
    "]\n",
    "names = ['rent_count', 'rent_mean', 'rent_geo_mean', 'rent_synthetic_lower_quartile', 'rent_synthetic_upper_quartile']\n",
    "frames = []\n",
    "for path, name in zip(paths, names):\n",
    "    f = pd.read_csv(path, dtype={'SAU': str})\n",
    "    frames.append(clean(f, name))\n",
    "    \n",
    "f = reduce(lambda x, y: pd.merge(x, y), frames)\n",
    "\n",
    "# Merge in region data\n",
    "path = DATA_DIR/'au2001.csv'\n",
    "g = pd.read_csv(path, dtype={'au2001': str})\n",
    "f = f.merge(g)\n",
    "\n",
    "# Write to file\n",
    "path = DATA_DIR/'rents.csv'\n",
    "f.to_csv(str(path), index=False)\n",
    "f[f['rent_count'].notnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explorer rents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = DATA_DIR/'rents.csv'\n",
    "f = pd.read_csv(path, dtype={'au2001': str})\n",
    "f.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice in time and aggregate \n",
    "\n",
    "def aggregate_rents(f, date, groupby_cols=('rental_area', '#bedrooms')):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cond = f['quarter'] >= date\n",
    "    f = f[cond].copy()\n",
    "    \n",
    "    def my_agg(group):\n",
    "        d = {}\n",
    "        d['territory'] = group['territory'].iat[0]\n",
    "        d['region'] = group['region'].iat[0]\n",
    "        d['rent_count'] = group['rent_count'].sum()\n",
    "        d['rent_mean'] = (group['rent_mean']*group['rent_count']).sum()/d['rent_count']\n",
    "        d['rent_geo_mean'] = (group['rent_geo_mean']**(group['rent_count']/d['rent_count'])).prod()\n",
    "        return pd.Series(d)\n",
    "\n",
    "    g = f.groupby(groupby_cols).apply(my_agg).reset_index()\n",
    "    return g\n",
    "\n",
    "agg_rents = aggregate_rents(f, '2016-12-01')\n",
    "agg_rents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = agg_rents['region'] == 'Canterbury'\n",
    "a = agg_rents[cond].copy()\n",
    "\n",
    "def hits(group):\n",
    "    d = {}\n",
    "    d['hit_frac'] = group['rent_count'].dropna().shape[0]/group['rent_count'].shape[0]\n",
    "    return pd.Series(d)\n",
    "\n",
    "a.groupby('#bedrooms').apply(hits).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose representative points for rental areas using property titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = DATA_DIR/'rental_areas.geojson'\n",
    "ra = gpd.read_file(str(path))\n",
    "\n",
    "path = DATA_DIR/'property_titles.geojson'\n",
    "t = gpd.read_file(str(path))\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f = gpd.sjoin(t[['geometry', 'fid']], ra, op='intersects')\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt(group):\n",
    "    d = {}\n",
    "    d['geometry'] = so.unary_union(group['geometry']).representative_point()\n",
    "    d['territory'] = group['territory'].iat[0]\n",
    "    d['region'] = group['region'].iat[0]\n",
    "    return pd.Series(d)\n",
    "\n",
    "g = gpd.GeoDataFrame(f.groupby('rental_area').apply(pt).reset_index())\n",
    "\n",
    "path = DATA_DIR/'rental_area_points.geojson'\n",
    "with path.open('w') as tgt:\n",
    "    tgt.write(g.to_json())\n",
    "\n",
    "g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[g['region'] == 'Auckland']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Mapzen to calculate time-distance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPZEN_ROUTE_URL = 'https://valhalla.mapzen.com'\n",
    "MAPZEN_MATRIX_URL = 'https://matrix.mapzen.com'\n",
    "MAPZEN_KEY = 'valhalla-Mc6zgDA'\n",
    "LOCAL_URL = 'http://localhost:8002'\n",
    "\n",
    "def get_route(origin, destination, mode, datetime=None, url=MAPZEN_ROUTE_URL, key=MAPZEN_KEY):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    url += '/route'\n",
    "    if datetime is None:\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%dT%H:%M')\n",
    "        \n",
    "    json_params = {\n",
    "        'locations': [\n",
    "            {'lon': origin[0], 'lat': origin[1]}, \n",
    "            {'lon': destination[0], 'lat': destination[1]},         \n",
    "        ],\n",
    "        'costing': mode,\n",
    "        'date_time': {'type':1, 'value': datetime},\n",
    "    }\n",
    "    params = {\n",
    "        'api_key': key,\n",
    "    }\n",
    "                \n",
    "    r = requests.get(url, json=json_params, params=params)\n",
    "\n",
    "    # Raise an error if bad request\n",
    "    r.raise_for_status()\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "def get_matrix(origins, destinations, mode, datetime=None, url=MAPZEN_MATRIX_URL, key=MAPZEN_KEY):\n",
    "    \"\"\" \n",
    "    Issue a GET request to the Valhalla time-distance matrix API at the given URL using the given API key.\n",
    "    Use the many-to-one option and the given origins (list of WGS84 longitude-latitude pairs) \n",
    "    and destination (list of WGS84 longitude-latitude pairs).\n",
    "    Return the (decoded) JSON response.\n",
    "    \n",
    "    Use the given mode of travel ('auto', 'bicycle', or 'pedestrian').\n",
    "    \n",
    "    NOTES:\n",
    "        - Raise an HTTP error if the request fails\n",
    "    \"\"\"\n",
    "    url += '/sources_to_targets'\n",
    "    sources = [{'lon': lon, 'lat': lat} for lon, lat in origins]\n",
    "    targets = [{'lon': lon, 'lat': lat} for lon, lat in destinations]\n",
    "    if datetime is None:\n",
    "        datetime = dt.datetime.now().strftime('%Y-%m-%dT%H:%M')\n",
    "        \n",
    "    json_params = {\n",
    "        'sources': sources,\n",
    "        'targets': targets,\n",
    "        'costing': mode,\n",
    "        'date_time': {'type':1, 'value': datetime},\n",
    "    }\n",
    "    params = {\n",
    "        'api_key': key,\n",
    "    }\n",
    "                \n",
    "    r = requests.get(url, json=json_params, params=params)\n",
    "\n",
    "    # Raise an error if bad request\n",
    "    r.raise_for_status()\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "def matrix_to_df(matrix, orig_names=None, dest_names=None):\n",
    "    \"\"\"\n",
    "    Given a (decoded) JSON time-distance matrix of the form output by :func:``get_matrix``, \n",
    "    a list of origin names (defaults to [0, 1, 2, etc.]), \n",
    "    and a list of destination names (defaults to [0, 1, 2, etc.]), convert the matrix to a DataFrame with\n",
    "    the columns:\n",
    "    \n",
    "    - ``'origin'``: one of ``orig_names``\n",
    "    - ``'destination'``: one of ``dest_names``\n",
    "    - ``'time'``: time from origin to destination\n",
    "    - ``'distance'``: distance from origin to destination\n",
    "    \n",
    "    The origin and destination names should be listed in the same order as the 'sources' and 'targets' \n",
    "    attributes of ``matrix``, respectively.\n",
    "    \"\"\"\n",
    "    # Build DataFrame\n",
    "    columns = ['from_index', 'to_index', 'distance', 'time']\n",
    "    rows = [[d[0][c] for c in columns] for d in matrix['sources_to_targets']]\n",
    "    f = pd.DataFrame(rows, columns=columns)\n",
    "    \n",
    "    # Map indices to names\n",
    "    if orig_names is not None:\n",
    "        orig_dict = dict(enumerate(orig_names))\n",
    "        f['origin'] = f['from_index'].map(orig_dict)\n",
    "    else:\n",
    "        f['origin'] = f['from_index']\n",
    "    if dest_names is not None:\n",
    "        dest_dict = dict(enumerate(dest_names))\n",
    "        f['destination'] = f['to_index'].map(dest_dict)\n",
    "    else:\n",
    "        f['destination'] = f['to_index']\n",
    "        \n",
    "    return f[['origin', 'destination', 'time', 'distance']].copy()\n",
    "    \n",
    "def collect_matrices(points, mode, datetime=None, chunk_size=49, url=MAPZEN_MATRIX_URL, key=MAPZEN_KEY):\n",
    "    \"\"\"\n",
    "    Call :func:`get_matrix` repeatedly using the meshblock centroids (GeoDataFrame) \n",
    "    as origins and the cities (GeoDataFrame) as destinations.\n",
    "    Only use meshblocks and cities within the same island (north or south), and\n",
    "    group the time-distance calls into ``chunk_size``-to-1 chunks. \n",
    "    Aggregate the result into one DataFrame of the form output by :func:``matrix_to_df`` \n",
    "    with meshblock IDs listed as origins and city names listed as destinations.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for __, row in points.iterrows():\n",
    "        dests = [row['geometry'].coords[0]]\n",
    "        ra = row['rental_area']\n",
    "        dest_names = [ra]\n",
    "        \n",
    "        # Chunk points and get matrix for each chuck to dests\n",
    "        num_chunks = math.ceil(points.shape[0]/chunk_size)\n",
    "        for g in np.array_split(points, num_chunks):\n",
    "            # Get origins\n",
    "            origs = [geo.coords[0] for geo in g['geometry']] \n",
    "            orig_names = g['rental_area'].values \n",
    "            # Get OD matrix\n",
    "            try:\n",
    "                j = get_matrix(origs, dests, mode=mode, datetime=datetime, url=url, key=key)\n",
    "                df = matrix_to_df(j, orig_names, dest_names)\n",
    "            except:\n",
    "                df = pd.DataFrame()\n",
    "                df['origin'] = orig_names\n",
    "                df['destination'] = ra\n",
    "                df['time'] = np.nan\n",
    "                df['distance'] = np.nan\n",
    "            frames.append(df)\n",
    "            \n",
    "    return pd.concat(frames).sort_values(['origin', 'destination'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': [[{'lat': -36.871899, 'lon': 174.779907}]],\n",
       " 'sources_to_targets': [[{'distance': 11.714,\n",
       "    'from_index': 0,\n",
       "    'time': 805,\n",
       "    'to_index': 0}]],\n",
       " 'targets': [[{'lat': -36.956001, 'lon': 174.786804}]],\n",
       " 'units': 'km'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test some\n",
    "orig = [ 174.7799, -36.8719]\n",
    "dest = [174.7868, -36.9560]\n",
    "datetime = '2017-05-12T08:00'\n",
    "#get_route(orig, dest, mode='multimodal', datetime=datetime)\n",
    "get_matrix([orig], [dest], mode='auto', datetime=datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>id</th>\n",
       "      <th>region</th>\n",
       "      <th>rental_area</th>\n",
       "      <th>territory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (174.7136142163085 -36.71740768433499)</td>\n",
       "      <td>2</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>Albany</td>\n",
       "      <td>North Shore City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>POINT (174.689014050193 -36.89612845095292)</td>\n",
       "      <td>6</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>Avondale</td>\n",
       "      <td>Auckland City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POINT (174.7513445789724 -36.89353312125105)</td>\n",
       "      <td>10</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>Balmoral</td>\n",
       "      <td>Auckland City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POINT (174.6988893987124 -36.79717233861077)</td>\n",
       "      <td>13</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>Beachhaven/Birkdale</td>\n",
       "      <td>North Shore City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>POINT (174.7059674167557 -36.91481011710743)</td>\n",
       "      <td>18</td>\n",
       "      <td>Auckland</td>\n",
       "      <td>Blockhouse Bay/New Windsor</td>\n",
       "      <td>Auckland City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        geometry  id    region  \\\n",
       "2   POINT (174.7136142163085 -36.71740768433499)   2  Auckland   \n",
       "6    POINT (174.689014050193 -36.89612845095292)   6  Auckland   \n",
       "10  POINT (174.7513445789724 -36.89353312125105)  10  Auckland   \n",
       "13  POINT (174.6988893987124 -36.79717233861077)  13  Auckland   \n",
       "18  POINT (174.7059674167557 -36.91481011710743)  18  Auckland   \n",
       "\n",
       "                   rental_area         territory  \n",
       "2                       Albany  North Shore City  \n",
       "6                     Avondale     Auckland City  \n",
       "10                    Balmoral     Auckland City  \n",
       "13         Beachhaven/Birkdale  North Shore City  \n",
       "18  Blockhouse Bay/New Windsor     Auckland City  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = DATA_DIR/'rental_area_points.geojson'\n",
    "f = gpd.read_file(str(path))\n",
    "f = f[f['region'] == 'Auckland'].copy()\n",
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.77 s, sys: 144 ms, total: 3.92 s\n",
      "Wall time: 8min 27s\n",
      "   origin                 destination     time  distance\n",
      "0  Albany                      Albany      0.0     0.000\n",
      "0  Albany                    Avondale  19269.0    28.841\n",
      "0  Albany                    Balmoral  16611.0    25.077\n",
      "0  Albany         Beachhaven/Birkdale   8438.0    11.914\n",
      "0  Albany  Blockhouse Bay/New Windsor  20089.0    29.993\n",
      "CPU times: user 3.89 s, sys: 228 ms, total: 4.12 s\n",
      "Wall time: 7min 58s\n",
      "   origin                 destination    time  distance\n",
      "0  Albany                      Albany     0.0     0.000\n",
      "0  Albany                    Avondale  4778.0    30.902\n",
      "0  Albany                    Balmoral  5616.0    36.069\n",
      "0  Albany         Beachhaven/Birkdale  1975.0    12.630\n",
      "0  Albany  Blockhouse Bay/New Windsor  5222.0    33.881\n",
      "CPU times: user 4.12 s, sys: 192 ms, total: 4.32 s\n",
      "Wall time: 5min\n",
      "   origin                 destination    time  distance\n",
      "0  Albany                      Albany     0.0     0.000\n",
      "0  Albany                    Avondale  1249.0    26.862\n",
      "0  Albany                    Balmoral  1243.0    24.671\n",
      "0  Albany         Beachhaven/Birkdale   877.0    13.707\n",
      "0  Albany  Blockhouse Bay/New Windsor  1433.0    28.855\n"
     ]
    }
   ],
   "source": [
    "# Multimodal time-distance matrices not implemented by Mapzen\n",
    "\n",
    "datetime = '2017-05-11T08:00'\n",
    "for mode in ['pedestrian', 'bicycle', 'auto']:\n",
    "    %time m = collect_matrices(f, mode=mode, datetime=datetime)\n",
    "    print(m.head())\n",
    "    path = DATA_DIR/'auckland'/'{!s}_matrix.csv'.format(mode)\n",
    "    m.to_csv(str(path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
